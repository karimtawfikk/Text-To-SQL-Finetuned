{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rDsCmsrmcxH"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from model_gguf/unsloth.Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Mistral7B Finetuned\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 7.2B\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  14:                           llama.vocab_size u32              = 32768\n",
            "llama_model_loader: - kv  15:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\n",
            "llama_model_loader: - kv  19:                      tokenizer.ggml.scores arr[f32,32768]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,32768]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  21:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  23:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 770\n",
            "llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  26:               tokenizer.ggml.add_sep_token bool             = false\n",
            "llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {%- if messages[0][\"role\"] == \"system...\n",
            "llama_model_loader: - kv  29:            tokenizer.ggml.add_space_prefix bool             = true\n",
            "llama_model_loader: - kv  30:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  31:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: control token:     48 '[control_46]' is not marked as EOG\n",
            "load: control token:    624 '[control_622]' is not marked as EOG\n",
            "load: control token:      0 '<unk>' is not marked as EOG\n",
            "load: control token:    216 '[control_214]' is not marked as EOG\n",
            "load: control token:     40 '[control_38]' is not marked as EOG\n",
            "load: control token:    322 '[control_320]' is not marked as EOG\n",
            "load: control token:      4 '[/INST]' is not marked as EOG\n",
            "load: control token:    366 '[control_364]' is not marked as EOG\n",
            "load: control token:     32 '[control_30]' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: control token:    425 '[control_423]' is not marked as EOG\n",
            "load: control token:     44 '[control_42]' is not marked as EOG\n",
            "load: control token:      7 '[/AVAILABLE_TOOLS]' is not marked as EOG\n",
            "load: control token:    206 '[control_204]' is not marked as EOG\n",
            "load: control token:     22 '[control_20]' is not marked as EOG\n",
            "load: control token:    241 '[control_239]' is not marked as EOG\n",
            "load: control token:    112 '[control_110]' is not marked as EOG\n",
            "load: control token:    398 '[control_396]' is not marked as EOG\n",
            "load: control token:      5 '[TOOL_CALLS]' is not marked as EOG\n",
            "load: control token:    655 '[control_653]' is not marked as EOG\n",
            "load: control token:    725 '[control_723]' is not marked as EOG\n",
            "load: control token:    340 '[control_338]' is not marked as EOG\n",
            "load: control token:    523 '[control_521]' is not marked as EOG\n",
            "load: control token:    242 '[control_240]' is not marked as EOG\n",
            "load: control token:      3 '[INST]' is not marked as EOG\n",
            "load: control token:    364 '[control_362]' is not marked as EOG\n",
            "load: control token:     38 '[control_36]' is not marked as EOG\n",
            "load: control token:    333 '[control_331]' is not marked as EOG\n",
            "load: control token:    530 '[control_528]' is not marked as EOG\n",
            "load: control token:    251 '[control_249]' is not marked as EOG\n",
            "load: control token:      6 '[AVAILABLE_TOOLS]' is not marked as EOG\n",
            "load: control token:     63 '[control_61]' is not marked as EOG\n",
            "load: control token:    346 '[control_344]' is not marked as EOG\n",
            "load: control token:     12 '[control_10]' is not marked as EOG\n",
            "load: control token:    409 '[control_407]' is not marked as EOG\n",
            "load: control token:      8 '[TOOL_RESULTS]' is not marked as EOG\n",
            "load: control token:    694 '[control_692]' is not marked as EOG\n",
            "load: control token:    153 '[control_151]' is not marked as EOG\n",
            "load: control token:    374 '[control_372]' is not marked as EOG\n",
            "load: control token:      9 '[/TOOL_RESULTS]' is not marked as EOG\n",
            "load: control token:     59 '[control_57]' is not marked as EOG\n",
            "load: control token:    191 '[control_189]' is not marked as EOG\n",
            "load: control token:    284 '[control_282]' is not marked as EOG\n",
            "load: control token:     10 '[control_8]' is not marked as EOG\n",
            "load: control token:     58 '[control_56]' is not marked as EOG\n",
            "load: control token:    190 '[control_188]' is not marked as EOG\n",
            "load: control token:    285 '[control_283]' is not marked as EOG\n",
            "load: control token:     11 '[control_9]' is not marked as EOG\n",
            "load: control token:     62 '[control_60]' is not marked as EOG\n",
            "load: control token:    347 '[control_345]' is not marked as EOG\n",
            "load: control token:     13 '[control_11]' is not marked as EOG\n",
            "load: control token:    348 '[control_346]' is not marked as EOG\n",
            "load: control token:     14 '[control_12]' is not marked as EOG\n",
            "load: control token:    349 '[control_347]' is not marked as EOG\n",
            "load: control token:     15 '[control_13]' is not marked as EOG\n",
            "load: control token:    698 '[control_696]' is not marked as EOG\n",
            "load: control token:    405 '[control_403]' is not marked as EOG\n",
            "load: control token:    157 '[control_155]' is not marked as EOG\n",
            "load: control token:     60 '[control_58]' is not marked as EOG\n",
            "load: control token:    342 '[control_340]' is not marked as EOG\n",
            "load: control token:     16 '[control_14]' is not marked as EOG\n",
            "load: control token:    699 '[control_697]' is not marked as EOG\n",
            "load: control token:    404 '[control_402]' is not marked as EOG\n",
            "load: control token:    156 '[control_154]' is not marked as EOG\n",
            "load: control token:     61 '[control_59]' is not marked as EOG\n",
            "load: control token:    343 '[control_341]' is not marked as EOG\n",
            "load: control token:     17 '[control_15]' is not marked as EOG\n",
            "load: control token:    344 '[control_342]' is not marked as EOG\n",
            "load: control token:     18 '[control_16]' is not marked as EOG\n",
            "load: control token:    345 '[control_343]' is not marked as EOG\n",
            "load: control token:     19 '[control_17]' is not marked as EOG\n",
            "load: control token:    161 '[control_159]' is not marked as EOG\n",
            "load: control token:     56 '[control_54]' is not marked as EOG\n",
            "load: control token:     20 '[control_18]' is not marked as EOG\n",
            "load: control token:     57 '[control_55]' is not marked as EOG\n",
            "load: control token:    160 '[control_158]' is not marked as EOG\n",
            "load: control token:     21 '[control_19]' is not marked as EOG\n",
            "load: control token:    207 '[control_205]' is not marked as EOG\n",
            "load: control token:     23 '[control_21]' is not marked as EOG\n",
            "load: control token:     24 '[control_22]' is not marked as EOG\n",
            "load: control token:    208 '[control_206]' is not marked as EOG\n",
            "load: control token:    209 '[control_207]' is not marked as EOG\n",
            "load: control token:     25 '[control_23]' is not marked as EOG\n",
            "load: control token:    380 '[control_378]' is not marked as EOG\n",
            "load: control token:     26 '[control_24]' is not marked as EOG\n",
            "load: control token:    202 '[control_200]' is not marked as EOG\n",
            "load: control token:    381 '[control_379]' is not marked as EOG\n",
            "load: control token:    203 '[control_201]' is not marked as EOG\n",
            "load: control token:     27 '[control_25]' is not marked as EOG\n",
            "load: control token:     28 '[control_26]' is not marked as EOG\n",
            "load: control token:    204 '[control_202]' is not marked as EOG\n",
            "load: control token:    205 '[control_203]' is not marked as EOG\n",
            "load: control token:     29 '[control_27]' is not marked as EOG\n",
            "load: control token:    376 '[control_374]' is not marked as EOG\n",
            "load: control token:     30 '[control_28]' is not marked as EOG\n",
            "load: control token:    377 '[control_375]' is not marked as EOG\n",
            "load: control token:     31 '[control_29]' is not marked as EOG\n",
            "load: control token:    367 '[control_365]' is not marked as EOG\n",
            "load: control token:     33 '[control_31]' is not marked as EOG\n",
            "load: control token:    368 '[control_366]' is not marked as EOG\n",
            "load: control token:     34 '[control_32]' is not marked as EOG\n",
            "load: control token:    369 '[control_367]' is not marked as EOG\n",
            "load: control token:     35 '[control_33]' is not marked as EOG\n",
            "load: control token:    362 '[control_360]' is not marked as EOG\n",
            "load: control token:    220 '[control_218]' is not marked as EOG\n",
            "load: control token:     36 '[control_34]' is not marked as EOG\n",
            "load: control token:    363 '[control_361]' is not marked as EOG\n",
            "load: control token:    221 '[control_219]' is not marked as EOG\n",
            "load: control token:     37 '[control_35]' is not marked as EOG\n",
            "load: control token:    365 '[control_363]' is not marked as EOG\n",
            "load: control token:     39 '[control_37]' is not marked as EOG\n",
            "load: control token:     41 '[control_39]' is not marked as EOG\n",
            "load: control token:    217 '[control_215]' is not marked as EOG\n",
            "load: control token:     42 '[control_40]' is not marked as EOG\n",
            "load: control token:     43 '[control_41]' is not marked as EOG\n",
            "load: control token:     45 '[control_43]' is not marked as EOG\n",
            "load: control token:    481 '[control_479]' is not marked as EOG\n",
            "load: control token:     46 '[control_44]' is not marked as EOG\n",
            "load: control token:    480 '[control_478]' is not marked as EOG\n",
            "load: control token:     47 '[control_45]' is not marked as EOG\n",
            "load: control token:     49 '[control_47]' is not marked as EOG\n",
            "load: control token:    477 '[control_475]' is not marked as EOG\n",
            "load: control token:     50 '[control_48]' is not marked as EOG\n",
            "load: control token:    476 '[control_474]' is not marked as EOG\n",
            "load: control token:     51 '[control_49]' is not marked as EOG\n",
            "load: control token:     52 '[control_50]' is not marked as EOG\n",
            "load: control token:     53 '[control_51]' is not marked as EOG\n",
            "load: control token:    411 '[control_409]' is not marked as EOG\n",
            "load: control token:     54 '[control_52]' is not marked as EOG\n",
            "load: control token:    410 '[control_408]' is not marked as EOG\n",
            "load: control token:     55 '[control_53]' is not marked as EOG\n",
            "load: control token:     64 '[control_62]' is not marked as EOG\n",
            "load: control token:     65 '[control_63]' is not marked as EOG\n",
            "load: control token:     66 '[control_64]' is not marked as EOG\n",
            "load: control token:     67 '[control_65]' is not marked as EOG\n",
            "load: control token:     68 '[control_66]' is not marked as EOG\n",
            "load: control token:     69 '[control_67]' is not marked as EOG\n",
            "load: control token:     70 '[control_68]' is not marked as EOG\n",
            "load: control token:     71 '[control_69]' is not marked as EOG\n",
            "load: control token:     72 '[control_70]' is not marked as EOG\n",
            "load: control token:     73 '[control_71]' is not marked as EOG\n",
            "load: control token:     74 '[control_72]' is not marked as EOG\n",
            "load: control token:     75 '[control_73]' is not marked as EOG\n",
            "load: control token:     76 '[control_74]' is not marked as EOG\n",
            "load: control token:     77 '[control_75]' is not marked as EOG\n",
            "load: control token:     78 '[control_76]' is not marked as EOG\n",
            "load: control token:     79 '[control_77]' is not marked as EOG\n",
            "load: control token:     80 '[control_78]' is not marked as EOG\n",
            "load: control token:     81 '[control_79]' is not marked as EOG\n",
            "load: control token:     82 '[control_80]' is not marked as EOG\n",
            "load: control token:     83 '[control_81]' is not marked as EOG\n",
            "load: control token:     84 '[control_82]' is not marked as EOG\n",
            "load: control token:     85 '[control_83]' is not marked as EOG\n",
            "load: control token:     86 '[control_84]' is not marked as EOG\n",
            "load: control token:     87 '[control_85]' is not marked as EOG\n",
            "load: control token:     88 '[control_86]' is not marked as EOG\n",
            "load: control token:     89 '[control_87]' is not marked as EOG\n",
            "load: control token:     90 '[control_88]' is not marked as EOG\n",
            "load: control token:     91 '[control_89]' is not marked as EOG\n",
            "load: control token:     92 '[control_90]' is not marked as EOG\n",
            "load: control token:     93 '[control_91]' is not marked as EOG\n",
            "load: control token:     94 '[control_92]' is not marked as EOG\n",
            "load: control token:     95 '[control_93]' is not marked as EOG\n",
            "load: control token:     96 '[control_94]' is not marked as EOG\n",
            "load: control token:     97 '[control_95]' is not marked as EOG\n",
            "load: control token:     98 '[control_96]' is not marked as EOG\n",
            "load: control token:     99 '[control_97]' is not marked as EOG\n",
            "load: control token:    100 '[control_98]' is not marked as EOG\n",
            "load: control token:    101 '[control_99]' is not marked as EOG\n",
            "load: control token:    454 '[control_452]' is not marked as EOG\n",
            "load: control token:    102 '[control_100]' is not marked as EOG\n",
            "load: control token:    455 '[control_453]' is not marked as EOG\n",
            "load: control token:    103 '[control_101]' is not marked as EOG\n",
            "load: control token:    452 '[control_450]' is not marked as EOG\n",
            "load: control token:    104 '[control_102]' is not marked as EOG\n",
            "load: control token:    390 '[control_388]' is not marked as EOG\n",
            "load: control token:    453 '[control_451]' is not marked as EOG\n",
            "load: control token:    105 '[control_103]' is not marked as EOG\n",
            "load: control token:    391 '[control_389]' is not marked as EOG\n",
            "load: control token:    458 '[control_456]' is not marked as EOG\n",
            "load: control token:    106 '[control_104]' is not marked as EOG\n",
            "load: control token:    459 '[control_457]' is not marked as EOG\n",
            "load: control token:    107 '[control_105]' is not marked as EOG\n",
            "load: control token:    456 '[control_454]' is not marked as EOG\n",
            "load: control token:    108 '[control_106]' is not marked as EOG\n",
            "load: control token:    457 '[control_455]' is not marked as EOG\n",
            "load: control token:    109 '[control_107]' is not marked as EOG\n",
            "load: control token:    110 '[control_108]' is not marked as EOG\n",
            "load: control token:    384 '[control_382]' is not marked as EOG\n",
            "load: control token:    111 '[control_109]' is not marked as EOG\n",
            "load: control token:    385 '[control_383]' is not marked as EOG\n",
            "load: control token:    240 '[control_238]' is not marked as EOG\n",
            "load: control token:    113 '[control_111]' is not marked as EOG\n",
            "load: control token:    399 '[control_397]' is not marked as EOG\n",
            "load: control token:    114 '[control_112]' is not marked as EOG\n",
            "load: control token:    396 '[control_394]' is not marked as EOG\n",
            "load: control token:    115 '[control_113]' is not marked as EOG\n",
            "load: control token:    397 '[control_395]' is not marked as EOG\n",
            "load: control token:    116 '[control_114]' is not marked as EOG\n",
            "load: control token:    394 '[control_392]' is not marked as EOG\n",
            "load: control token:    395 '[control_393]' is not marked as EOG\n",
            "load: control token:    117 '[control_115]' is not marked as EOG\n",
            "load: control token:    450 '[control_448]' is not marked as EOG\n",
            "load: control token:    657 '[control_655]' is not marked as EOG\n",
            "load: control token:    118 '[control_116]' is not marked as EOG\n",
            "load: control token:    392 '[control_390]' is not marked as EOG\n",
            "load: control token:    656 '[control_654]' is not marked as EOG\n",
            "load: control token:    451 '[control_449]' is not marked as EOG\n",
            "load: control token:    119 '[control_117]' is not marked as EOG\n",
            "load: control token:    393 '[control_391]' is not marked as EOG\n",
            "load: control token:    448 '[control_446]' is not marked as EOG\n",
            "load: control token:    120 '[control_118]' is not marked as EOG\n",
            "load: control token:    233 '[control_231]' is not marked as EOG\n",
            "load: control token:    449 '[control_447]' is not marked as EOG\n",
            "load: control token:    121 '[control_119]' is not marked as EOG\n",
            "load: control token:    232 '[control_230]' is not marked as EOG\n",
            "load: control token:    122 '[control_120]' is not marked as EOG\n",
            "load: control token:    123 '[control_121]' is not marked as EOG\n",
            "load: control token:    124 '[control_122]' is not marked as EOG\n",
            "load: control token:    125 '[control_123]' is not marked as EOG\n",
            "load: control token:    126 '[control_124]' is not marked as EOG\n",
            "load: control token:    127 '[control_125]' is not marked as EOG\n",
            "load: control token:    510 '[control_508]' is not marked as EOG\n",
            "load: control token:    231 '[control_229]' is not marked as EOG\n",
            "load: control token:    128 '[control_126]' is not marked as EOG\n",
            "load: control token:    129 '[control_127]' is not marked as EOG\n",
            "load: control token:    230 '[control_228]' is not marked as EOG\n",
            "load: control token:    511 '[control_509]' is not marked as EOG\n",
            "load: control token:    130 '[control_128]' is not marked as EOG\n",
            "load: control token:    229 '[control_227]' is not marked as EOG\n",
            "load: control token:    508 '[control_506]' is not marked as EOG\n",
            "load: control token:    131 '[control_129]' is not marked as EOG\n",
            "load: control token:    228 '[control_226]' is not marked as EOG\n",
            "load: control token:    509 '[control_507]' is not marked as EOG\n",
            "load: control token:    132 '[control_130]' is not marked as EOG\n",
            "load: control token:    261 '[control_259]' is not marked as EOG\n",
            "load: control token:    133 '[control_131]' is not marked as EOG\n",
            "load: control token:    260 '[control_258]' is not marked as EOG\n",
            "load: control token:    520 '[control_518]' is not marked as EOG\n",
            "load: control token:    134 '[control_132]' is not marked as EOG\n",
            "load: control token:    135 '[control_133]' is not marked as EOG\n",
            "load: control token:    521 '[control_519]' is not marked as EOG\n",
            "load: control token:    136 '[control_134]' is not marked as EOG\n",
            "load: control token:    137 '[control_135]' is not marked as EOG\n",
            "load: control token:    138 '[control_136]' is not marked as EOG\n",
            "load: control token:    139 '[control_137]' is not marked as EOG\n",
            "load: control token:    514 '[control_512]' is not marked as EOG\n",
            "load: control token:    140 '[control_138]' is not marked as EOG\n",
            "load: control token:    253 '[control_251]' is not marked as EOG\n",
            "load: control token:    515 '[control_513]' is not marked as EOG\n",
            "load: control token:    141 '[control_139]' is not marked as EOG\n",
            "load: control token:    252 '[control_250]' is not marked as EOG\n",
            "load: control token:    414 '[control_412]' is not marked as EOG\n",
            "load: control token:    142 '[control_140]' is not marked as EOG\n",
            "load: control token:    415 '[control_413]' is not marked as EOG\n",
            "load: control token:    143 '[control_141]' is not marked as EOG\n",
            "load: control token:    412 '[control_410]' is not marked as EOG\n",
            "load: control token:    144 '[control_142]' is not marked as EOG\n",
            "load: control token:    413 '[control_411]' is not marked as EOG\n",
            "load: control token:    145 '[control_143]' is not marked as EOG\n",
            "load: control token:    418 '[control_416]' is not marked as EOG\n",
            "load: control token:    146 '[control_144]' is not marked as EOG\n",
            "load: control token:    419 '[control_417]' is not marked as EOG\n",
            "load: control token:    147 '[control_145]' is not marked as EOG\n",
            "load: control token:    416 '[control_414]' is not marked as EOG\n",
            "load: control token:    148 '[control_146]' is not marked as EOG\n",
            "load: control token:    417 '[control_415]' is not marked as EOG\n",
            "load: control token:    149 '[control_147]' is not marked as EOG\n",
            "load: control token:    150 '[control_148]' is not marked as EOG\n",
            "load: control token:    151 '[control_149]' is not marked as EOG\n",
            "load: control token:    695 '[control_693]' is not marked as EOG\n",
            "load: control token:    408 '[control_406]' is not marked as EOG\n",
            "load: control token:    152 '[control_150]' is not marked as EOG\n",
            "load: control token:    406 '[control_404]' is not marked as EOG\n",
            "load: control token:    693 '[control_691]' is not marked as EOG\n",
            "load: control token:    154 '[control_152]' is not marked as EOG\n",
            "load: control token:    692 '[control_690]' is not marked as EOG\n",
            "load: control token:    407 '[control_405]' is not marked as EOG\n",
            "load: control token:    155 '[control_153]' is not marked as EOG\n",
            "load: control token:    697 '[control_695]' is not marked as EOG\n",
            "load: control token:    402 '[control_400]' is not marked as EOG\n",
            "load: control token:    158 '[control_156]' is not marked as EOG\n",
            "load: control token:    696 '[control_694]' is not marked as EOG\n",
            "load: control token:    403 '[control_401]' is not marked as EOG\n",
            "load: control token:    159 '[control_157]' is not marked as EOG\n",
            "load: control token:    162 '[control_160]' is not marked as EOG\n",
            "load: control token:    163 '[control_161]' is not marked as EOG\n",
            "load: control token:    164 '[control_162]' is not marked as EOG\n",
            "load: control token:    165 '[control_163]' is not marked as EOG\n",
            "load: control token:    166 '[control_164]' is not marked as EOG\n",
            "load: control token:    167 '[control_165]' is not marked as EOG\n",
            "load: control token:    761 '[control_759]' is not marked as EOG\n",
            "load: control token:    168 '[control_166]' is not marked as EOG\n",
            "load: control token:    760 '[control_758]' is not marked as EOG\n",
            "load: control token:    169 '[control_167]' is not marked as EOG\n",
            "load: control token:    759 '[control_757]' is not marked as EOG\n",
            "load: control token:    170 '[control_168]' is not marked as EOG\n",
            "load: control token:    758 '[control_756]' is not marked as EOG\n",
            "load: control token:    171 '[control_169]' is not marked as EOG\n",
            "load: control token:    464 '[control_462]' is not marked as EOG\n",
            "load: control token:    172 '[control_170]' is not marked as EOG\n",
            "load: control token:    465 '[control_463]' is not marked as EOG\n",
            "load: control token:    173 '[control_171]' is not marked as EOG\n",
            "load: control token:    462 '[control_460]' is not marked as EOG\n",
            "load: control token:    174 '[control_172]' is not marked as EOG\n",
            "load: control token:    463 '[control_461]' is not marked as EOG\n",
            "load: control token:    175 '[control_173]' is not marked as EOG\n",
            "load: control token:    468 '[control_466]' is not marked as EOG\n",
            "load: control token:    176 '[control_174]' is not marked as EOG\n",
            "load: control token:    469 '[control_467]' is not marked as EOG\n",
            "load: control token:    177 '[control_175]' is not marked as EOG\n",
            "load: control token:    466 '[control_464]' is not marked as EOG\n",
            "load: control token:    178 '[control_176]' is not marked as EOG\n",
            "load: control token:    467 '[control_465]' is not marked as EOG\n",
            "load: control token:    179 '[control_177]' is not marked as EOG\n",
            "load: control token:    180 '[control_178]' is not marked as EOG\n",
            "load: control token:    181 '[control_179]' is not marked as EOG\n",
            "load: control token:    182 '[control_180]' is not marked as EOG\n",
            "load: control token:    183 '[control_181]' is not marked as EOG\n",
            "load: control token:    184 '[control_182]' is not marked as EOG\n",
            "load: control token:    291 '[control_289]' is not marked as EOG\n",
            "load: control token:    185 '[control_183]' is not marked as EOG\n",
            "load: control token:    290 '[control_288]' is not marked as EOG\n",
            "load: control token:    186 '[control_184]' is not marked as EOG\n",
            "load: control token:    187 '[control_185]' is not marked as EOG\n",
            "load: control token:    188 '[control_186]' is not marked as EOG\n",
            "load: control token:    189 '[control_187]' is not marked as EOG\n",
            "load: control token:    192 '[control_190]' is not marked as EOG\n",
            "load: control token:    318 '[control_316]' is not marked as EOG\n",
            "load: control token:    193 '[control_191]' is not marked as EOG\n",
            "load: control token:    319 '[control_317]' is not marked as EOG\n",
            "load: control token:    194 '[control_192]' is not marked as EOG\n",
            "load: control token:    316 '[control_314]' is not marked as EOG\n",
            "load: control token:    195 '[control_193]' is not marked as EOG\n",
            "load: control token:    317 '[control_315]' is not marked as EOG\n",
            "load: control token:    196 '[control_194]' is not marked as EOG\n",
            "load: control token:    314 '[control_312]' is not marked as EOG\n",
            "load: control token:    315 '[control_313]' is not marked as EOG\n",
            "load: control token:    197 '[control_195]' is not marked as EOG\n",
            "load: control token:    198 '[control_196]' is not marked as EOG\n",
            "load: control token:    312 '[control_310]' is not marked as EOG\n",
            "load: control token:    551 '[control_549]' is not marked as EOG\n",
            "load: control token:    270 '[control_268]' is not marked as EOG\n",
            "load: control token:    199 '[control_197]' is not marked as EOG\n",
            "load: control token:    313 '[control_311]' is not marked as EOG\n",
            "load: control token:    550 '[control_548]' is not marked as EOG\n",
            "load: control token:    271 '[control_269]' is not marked as EOG\n",
            "load: control token:    268 '[control_266]' is not marked as EOG\n",
            "load: control token:    549 '[control_547]' is not marked as EOG\n",
            "load: control token:    200 '[control_198]' is not marked as EOG\n",
            "load: control token:    548 '[control_546]' is not marked as EOG\n",
            "load: control token:    269 '[control_267]' is not marked as EOG\n",
            "load: control token:    201 '[control_199]' is not marked as EOG\n",
            "load: control token:    372 '[control_370]' is not marked as EOG\n",
            "load: control token:    210 '[control_208]' is not marked as EOG\n",
            "load: control token:    373 '[control_371]' is not marked as EOG\n",
            "load: control token:    211 '[control_209]' is not marked as EOG\n",
            "load: control token:    370 '[control_368]' is not marked as EOG\n",
            "load: control token:    212 '[control_210]' is not marked as EOG\n",
            "load: control token:    371 '[control_369]' is not marked as EOG\n",
            "load: control token:    213 '[control_211]' is not marked as EOG\n",
            "load: control token:    214 '[control_212]' is not marked as EOG\n",
            "load: control token:    215 '[control_213]' is not marked as EOG\n",
            "load: control token:    218 '[control_216]' is not marked as EOG\n",
            "load: control token:    219 '[control_217]' is not marked as EOG\n",
            "load: control token:    222 '[control_220]' is not marked as EOG\n",
            "load: control token:    503 '[control_501]' is not marked as EOG\n",
            "load: control token:    502 '[control_500]' is not marked as EOG\n",
            "load: control token:    223 '[control_221]' is not marked as EOG\n",
            "load: control token:    224 '[control_222]' is not marked as EOG\n",
            "load: control token:    505 '[control_503]' is not marked as EOG\n",
            "load: control token:    504 '[control_502]' is not marked as EOG\n",
            "load: control token:    225 '[control_223]' is not marked as EOG\n",
            "load: control token:    226 '[control_224]' is not marked as EOG\n",
            "load: control token:    507 '[control_505]' is not marked as EOG\n",
            "load: control token:    227 '[control_225]' is not marked as EOG\n",
            "load: control token:    506 '[control_504]' is not marked as EOG\n",
            "load: control token:    660 '[control_658]' is not marked as EOG\n",
            "load: control token:    447 '[control_445]' is not marked as EOG\n",
            "load: control token:    234 '[control_232]' is not marked as EOG\n",
            "load: control token:    661 '[control_659]' is not marked as EOG\n",
            "load: control token:    446 '[control_444]' is not marked as EOG\n",
            "load: control token:    235 '[control_233]' is not marked as EOG\n",
            "load: control token:    445 '[control_443]' is not marked as EOG\n",
            "load: control token:    236 '[control_234]' is not marked as EOG\n",
            "load: control token:    444 '[control_442]' is not marked as EOG\n",
            "load: control token:    237 '[control_235]' is not marked as EOG\n",
            "load: control token:    443 '[control_441]' is not marked as EOG\n",
            "load: control token:    238 '[control_236]' is not marked as EOG\n",
            "load: control token:    401 '[control_399]' is not marked as EOG\n",
            "load: control token:    442 '[control_440]' is not marked as EOG\n",
            "load: control token:    239 '[control_237]' is not marked as EOG\n",
            "load: control token:    400 '[control_398]' is not marked as EOG\n",
            "load: control token:    341 '[control_339]' is not marked as EOG\n",
            "load: control token:    724 '[control_722]' is not marked as EOG\n",
            "load: control token:    522 '[control_520]' is not marked as EOG\n",
            "load: control token:    243 '[control_241]' is not marked as EOG\n",
            "load: control token:    525 '[control_523]' is not marked as EOG\n",
            "load: control token:    244 '[control_242]' is not marked as EOG\n",
            "load: control token:    245 '[control_243]' is not marked as EOG\n",
            "load: control token:    524 '[control_522]' is not marked as EOG\n",
            "load: control token:    246 '[control_244]' is not marked as EOG\n",
            "load: control token:    527 '[control_525]' is not marked as EOG\n",
            "load: control token:    526 '[control_524]' is not marked as EOG\n",
            "load: control token:    247 '[control_245]' is not marked as EOG\n",
            "load: control token:    529 '[control_527]' is not marked as EOG\n",
            "load: control token:    248 '[control_246]' is not marked as EOG\n",
            "load: control token:    249 '[control_247]' is not marked as EOG\n",
            "load: control token:    528 '[control_526]' is not marked as EOG\n",
            "load: control token:    332 '[control_330]' is not marked as EOG\n",
            "load: control token:    531 '[control_529]' is not marked as EOG\n",
            "load: control token:    250 '[control_248]' is not marked as EOG\n",
            "load: control token:    254 '[control_252]' is not marked as EOG\n",
            "load: control token:    513 '[control_511]' is not marked as EOG\n",
            "load: control token:    255 '[control_253]' is not marked as EOG\n",
            "load: control token:    512 '[control_510]' is not marked as EOG\n",
            "load: control token:    519 '[control_517]' is not marked as EOG\n",
            "load: control token:    256 '[control_254]' is not marked as EOG\n",
            "load: control token:    518 '[control_516]' is not marked as EOG\n",
            "load: control token:    257 '[control_255]' is not marked as EOG\n",
            "load: control token:    517 '[control_515]' is not marked as EOG\n",
            "load: control token:    258 '[control_256]' is not marked as EOG\n",
            "load: control token:    516 '[control_514]' is not marked as EOG\n",
            "load: control token:    259 '[control_257]' is not marked as EOG\n",
            "load: control token:    320 '[control_318]' is not marked as EOG\n",
            "load: control token:    543 '[control_541]' is not marked as EOG\n",
            "load: control token:    262 '[control_260]' is not marked as EOG\n",
            "load: control token:    321 '[control_319]' is not marked as EOG\n",
            "load: control token:    542 '[control_540]' is not marked as EOG\n",
            "load: control token:    263 '[control_261]' is not marked as EOG\n",
            "load: control token:    264 '[control_262]' is not marked as EOG\n",
            "load: control token:    545 '[control_543]' is not marked as EOG\n",
            "load: control token:    544 '[control_542]' is not marked as EOG\n",
            "load: control token:    265 '[control_263]' is not marked as EOG\n",
            "load: control token:    547 '[control_545]' is not marked as EOG\n",
            "load: control token:    266 '[control_264]' is not marked as EOG\n",
            "load: control token:    267 '[control_265]' is not marked as EOG\n",
            "load: control token:    546 '[control_544]' is not marked as EOG\n",
            "load: control token:    310 '[control_308]' is not marked as EOG\n",
            "load: control token:    733 '[control_731]' is not marked as EOG\n",
            "load: control token:    539 '[control_537]' is not marked as EOG\n",
            "load: control token:    272 '[control_270]' is not marked as EOG\n",
            "load: control token:    311 '[control_309]' is not marked as EOG\n",
            "load: control token:    732 '[control_730]' is not marked as EOG\n",
            "load: control token:    273 '[control_271]' is not marked as EOG\n",
            "load: control token:    538 '[control_536]' is not marked as EOG\n",
            "load: control token:    537 '[control_535]' is not marked as EOG\n",
            "load: control token:    274 '[control_272]' is not marked as EOG\n",
            "load: control token:    536 '[control_534]' is not marked as EOG\n",
            "load: control token:    275 '[control_273]' is not marked as EOG\n",
            "load: control token:    535 '[control_533]' is not marked as EOG\n",
            "load: control token:    276 '[control_274]' is not marked as EOG\n",
            "load: control token:    534 '[control_532]' is not marked as EOG\n",
            "load: control token:    277 '[control_275]' is not marked as EOG\n",
            "load: control token:    533 '[control_531]' is not marked as EOG\n",
            "load: control token:    278 '[control_276]' is not marked as EOG\n",
            "load: control token:    532 '[control_530]' is not marked as EOG\n",
            "load: control token:    279 '[control_277]' is not marked as EOG\n",
            "load: control token:    741 '[control_739]' is not marked as EOG\n",
            "load: control token:    302 '[control_300]' is not marked as EOG\n",
            "load: control token:    280 '[control_278]' is not marked as EOG\n",
            "load: control token:    303 '[control_301]' is not marked as EOG\n",
            "load: control token:    740 '[control_738]' is not marked as EOG\n",
            "load: control token:    281 '[control_279]' is not marked as EOG\n",
            "load: control token:    282 '[control_280]' is not marked as EOG\n",
            "load: control token:    283 '[control_281]' is not marked as EOG\n",
            "load: control token:    286 '[control_284]' is not marked as EOG\n",
            "load: control token:    287 '[control_285]' is not marked as EOG\n",
            "load: control token:    288 '[control_286]' is not marked as EOG\n",
            "load: control token:    289 '[control_287]' is not marked as EOG\n",
            "load: control token:    292 '[control_290]' is not marked as EOG\n",
            "load: control token:    293 '[control_291]' is not marked as EOG\n",
            "load: control token:    294 '[control_292]' is not marked as EOG\n",
            "load: control token:    295 '[control_293]' is not marked as EOG\n",
            "load: control token:    296 '[control_294]' is not marked as EOG\n",
            "load: control token:    297 '[control_295]' is not marked as EOG\n",
            "load: control token:    298 '[control_296]' is not marked as EOG\n",
            "load: control token:    299 '[control_297]' is not marked as EOG\n",
            "load: control token:    300 '[control_298]' is not marked as EOG\n",
            "load: control token:    301 '[control_299]' is not marked as EOG\n",
            "load: control token:    304 '[control_302]' is not marked as EOG\n",
            "load: control token:    305 '[control_303]' is not marked as EOG\n",
            "load: control token:    306 '[control_304]' is not marked as EOG\n",
            "load: control token:    307 '[control_305]' is not marked as EOG\n",
            "load: control token:    308 '[control_306]' is not marked as EOG\n",
            "load: control token:    309 '[control_307]' is not marked as EOG\n",
            "load: control token:    323 '[control_321]' is not marked as EOG\n",
            "load: control token:    324 '[control_322]' is not marked as EOG\n",
            "load: control token:    325 '[control_323]' is not marked as EOG\n",
            "load: control token:    326 '[control_324]' is not marked as EOG\n",
            "load: control token:    327 '[control_325]' is not marked as EOG\n",
            "load: control token:    328 '[control_326]' is not marked as EOG\n",
            "load: control token:    329 '[control_327]' is not marked as EOG\n",
            "load: control token:    330 '[control_328]' is not marked as EOG\n",
            "load: control token:    331 '[control_329]' is not marked as EOG\n",
            "load: control token:    334 '[control_332]' is not marked as EOG\n",
            "load: control token:    731 '[control_729]' is not marked as EOG\n",
            "load: control token:    730 '[control_728]' is not marked as EOG\n",
            "load: control token:    335 '[control_333]' is not marked as EOG\n",
            "load: control token:    336 '[control_334]' is not marked as EOG\n",
            "load: control token:    337 '[control_335]' is not marked as EOG\n",
            "load: control token:    338 '[control_336]' is not marked as EOG\n",
            "load: control token:    339 '[control_337]' is not marked as EOG\n",
            "load: control token:    350 '[control_348]' is not marked as EOG\n",
            "load: control token:    351 '[control_349]' is not marked as EOG\n",
            "load: control token:    352 '[control_350]' is not marked as EOG\n",
            "load: control token:    353 '[control_351]' is not marked as EOG\n",
            "load: control token:    354 '[control_352]' is not marked as EOG\n",
            "load: control token:    355 '[control_353]' is not marked as EOG\n",
            "load: control token:    356 '[control_354]' is not marked as EOG\n",
            "load: control token:    357 '[control_355]' is not marked as EOG\n",
            "load: control token:    358 '[control_356]' is not marked as EOG\n",
            "load: control token:    359 '[control_357]' is not marked as EOG\n",
            "load: control token:    360 '[control_358]' is not marked as EOG\n",
            "load: control token:    361 '[control_359]' is not marked as EOG\n",
            "load: control token:    375 '[control_373]' is not marked as EOG\n",
            "load: control token:    378 '[control_376]' is not marked as EOG\n",
            "load: control token:    379 '[control_377]' is not marked as EOG\n",
            "load: control token:    460 '[control_458]' is not marked as EOG\n",
            "load: control token:    382 '[control_380]' is not marked as EOG\n",
            "load: control token:    461 '[control_459]' is not marked as EOG\n",
            "load: control token:    383 '[control_381]' is not marked as EOG\n",
            "load: control token:    386 '[control_384]' is not marked as EOG\n",
            "load: control token:    387 '[control_385]' is not marked as EOG\n",
            "load: control token:    388 '[control_386]' is not marked as EOG\n",
            "load: control token:    389 '[control_387]' is not marked as EOG\n",
            "load: control token:    420 '[control_418]' is not marked as EOG\n",
            "load: control token:    421 '[control_419]' is not marked as EOG\n",
            "load: control token:    422 '[control_420]' is not marked as EOG\n",
            "load: control token:    423 '[control_421]' is not marked as EOG\n",
            "load: control token:    424 '[control_422]' is not marked as EOG\n",
            "load: control token:    426 '[control_424]' is not marked as EOG\n",
            "load: control token:    427 '[control_425]' is not marked as EOG\n",
            "load: control token:    428 '[control_426]' is not marked as EOG\n",
            "load: control token:    429 '[control_427]' is not marked as EOG\n",
            "load: control token:    430 '[control_428]' is not marked as EOG\n",
            "load: control token:    431 '[control_429]' is not marked as EOG\n",
            "load: control token:    432 '[control_430]' is not marked as EOG\n",
            "load: control token:    433 '[control_431]' is not marked as EOG\n",
            "load: control token:    434 '[control_432]' is not marked as EOG\n",
            "load: control token:    435 '[control_433]' is not marked as EOG\n",
            "load: control token:    436 '[control_434]' is not marked as EOG\n",
            "load: control token:    437 '[control_435]' is not marked as EOG\n",
            "load: control token:    438 '[control_436]' is not marked as EOG\n",
            "load: control token:    439 '[control_437]' is not marked as EOG\n",
            "load: control token:    440 '[control_438]' is not marked as EOG\n",
            "load: control token:    441 '[control_439]' is not marked as EOG\n",
            "load: control token:    470 '[control_468]' is not marked as EOG\n",
            "load: control token:    471 '[control_469]' is not marked as EOG\n",
            "load: control token:    472 '[control_470]' is not marked as EOG\n",
            "load: control token:    473 '[control_471]' is not marked as EOG\n",
            "load: control token:    474 '[control_472]' is not marked as EOG\n",
            "load: control token:    475 '[control_473]' is not marked as EOG\n",
            "load: control token:    478 '[control_476]' is not marked as EOG\n",
            "load: control token:    479 '[control_477]' is not marked as EOG\n",
            "load: control token:    617 '[control_615]' is not marked as EOG\n",
            "load: control token:    482 '[control_480]' is not marked as EOG\n",
            "load: control token:    616 '[control_614]' is not marked as EOG\n",
            "load: control token:    483 '[control_481]' is not marked as EOG\n",
            "load: control token:    619 '[control_617]' is not marked as EOG\n",
            "load: control token:    484 '[control_482]' is not marked as EOG\n",
            "load: control token:    618 '[control_616]' is not marked as EOG\n",
            "load: control token:    485 '[control_483]' is not marked as EOG\n",
            "load: control token:    613 '[control_611]' is not marked as EOG\n",
            "load: control token:    486 '[control_484]' is not marked as EOG\n",
            "load: control token:    612 '[control_610]' is not marked as EOG\n",
            "load: control token:    487 '[control_485]' is not marked as EOG\n",
            "load: control token:    615 '[control_613]' is not marked as EOG\n",
            "load: control token:    488 '[control_486]' is not marked as EOG\n",
            "load: control token:    489 '[control_487]' is not marked as EOG\n",
            "load: control token:    614 '[control_612]' is not marked as EOG\n",
            "load: control token:    490 '[control_488]' is not marked as EOG\n",
            "load: control token:    491 '[control_489]' is not marked as EOG\n",
            "load: control token:    665 '[control_663]' is not marked as EOG\n",
            "load: control token:    492 '[control_490]' is not marked as EOG\n",
            "load: control token:    664 '[control_662]' is not marked as EOG\n",
            "load: control token:    493 '[control_491]' is not marked as EOG\n",
            "load: control token:    663 '[control_661]' is not marked as EOG\n",
            "load: control token:    494 '[control_492]' is not marked as EOG\n",
            "load: control token:    662 '[control_660]' is not marked as EOG\n",
            "load: control token:    495 '[control_493]' is not marked as EOG\n",
            "load: control token:    669 '[control_667]' is not marked as EOG\n",
            "load: control token:    496 '[control_494]' is not marked as EOG\n",
            "load: control token:    668 '[control_666]' is not marked as EOG\n",
            "load: control token:    497 '[control_495]' is not marked as EOG\n",
            "load: control token:    667 '[control_665]' is not marked as EOG\n",
            "load: control token:    498 '[control_496]' is not marked as EOG\n",
            "load: control token:    666 '[control_664]' is not marked as EOG\n",
            "load: control token:    499 '[control_497]' is not marked as EOG\n",
            "load: control token:    500 '[control_498]' is not marked as EOG\n",
            "load: control token:    501 '[control_499]' is not marked as EOG\n",
            "load: control token:    540 '[control_538]' is not marked as EOG\n",
            "load: control token:    541 '[control_539]' is not marked as EOG\n",
            "load: control token:    552 '[control_550]' is not marked as EOG\n",
            "load: control token:    553 '[control_551]' is not marked as EOG\n",
            "load: control token:    554 '[control_552]' is not marked as EOG\n",
            "load: control token:    555 '[control_553]' is not marked as EOG\n",
            "load: control token:    556 '[control_554]' is not marked as EOG\n",
            "load: control token:    557 '[control_555]' is not marked as EOG\n",
            "load: control token:    558 '[control_556]' is not marked as EOG\n",
            "load: control token:    559 '[control_557]' is not marked as EOG\n",
            "load: control token:    560 '[control_558]' is not marked as EOG\n",
            "load: control token:    561 '[control_559]' is not marked as EOG\n",
            "load: control token:    562 '[control_560]' is not marked as EOG\n",
            "load: control token:    563 '[control_561]' is not marked as EOG\n",
            "load: control token:    564 '[control_562]' is not marked as EOG\n",
            "load: control token:    565 '[control_563]' is not marked as EOG\n",
            "load: control token:    566 '[control_564]' is not marked as EOG\n",
            "load: control token:    567 '[control_565]' is not marked as EOG\n",
            "load: control token:    568 '[control_566]' is not marked as EOG\n",
            "load: control token:    569 '[control_567]' is not marked as EOG\n",
            "load: control token:    570 '[control_568]' is not marked as EOG\n",
            "load: control token:    571 '[control_569]' is not marked as EOG\n",
            "load: control token:    572 '[control_570]' is not marked as EOG\n",
            "load: control token:    573 '[control_571]' is not marked as EOG\n",
            "load: control token:    574 '[control_572]' is not marked as EOG\n",
            "load: control token:    575 '[control_573]' is not marked as EOG\n",
            "load: control token:    576 '[control_574]' is not marked as EOG\n",
            "load: control token:    577 '[control_575]' is not marked as EOG\n",
            "load: control token:    578 '[control_576]' is not marked as EOG\n",
            "load: control token:    579 '[control_577]' is not marked as EOG\n",
            "load: control token:    580 '[control_578]' is not marked as EOG\n",
            "load: control token:    581 '[control_579]' is not marked as EOG\n",
            "load: control token:    582 '[control_580]' is not marked as EOG\n",
            "load: control token:    583 '[control_581]' is not marked as EOG\n",
            "load: control token:    584 '[control_582]' is not marked as EOG\n",
            "load: control token:    585 '[control_583]' is not marked as EOG\n",
            "load: control token:    586 '[control_584]' is not marked as EOG\n",
            "load: control token:    587 '[control_585]' is not marked as EOG\n",
            "load: control token:    588 '[control_586]' is not marked as EOG\n",
            "load: control token:    589 '[control_587]' is not marked as EOG\n",
            "load: control token:    590 '[control_588]' is not marked as EOG\n",
            "load: control token:    591 '[control_589]' is not marked as EOG\n",
            "load: control token:    592 '[control_590]' is not marked as EOG\n",
            "load: control token:    593 '[control_591]' is not marked as EOG\n",
            "load: control token:    594 '[control_592]' is not marked as EOG\n",
            "load: control token:    595 '[control_593]' is not marked as EOG\n",
            "load: control token:    596 '[control_594]' is not marked as EOG\n",
            "load: control token:    690 '[control_688]' is not marked as EOG\n",
            "load: control token:    597 '[control_595]' is not marked as EOG\n",
            "load: control token:    691 '[control_689]' is not marked as EOG\n",
            "load: control token:    598 '[control_596]' is not marked as EOG\n",
            "load: control token:    599 '[control_597]' is not marked as EOG\n",
            "load: control token:    600 '[control_598]' is not marked as EOG\n",
            "load: control token:    686 '[control_684]' is not marked as EOG\n",
            "load: control token:    601 '[control_599]' is not marked as EOG\n",
            "load: control token:    687 '[control_685]' is not marked as EOG\n",
            "load: control token:    711 '[control_709]' is not marked as EOG\n",
            "load: control token:    602 '[control_600]' is not marked as EOG\n",
            "load: control token:    710 '[control_708]' is not marked as EOG\n",
            "load: control token:    603 '[control_601]' is not marked as EOG\n",
            "load: control token:    604 '[control_602]' is not marked as EOG\n",
            "load: control token:    605 '[control_603]' is not marked as EOG\n",
            "load: control token:    606 '[control_604]' is not marked as EOG\n",
            "load: control token:    607 '[control_605]' is not marked as EOG\n",
            "load: control token:    608 '[control_606]' is not marked as EOG\n",
            "load: control token:    609 '[control_607]' is not marked as EOG\n",
            "load: control token:    703 '[control_701]' is not marked as EOG\n",
            "load: control token:    610 '[control_608]' is not marked as EOG\n",
            "load: control token:    702 '[control_700]' is not marked as EOG\n",
            "load: control token:    611 '[control_609]' is not marked as EOG\n",
            "load: control token:    620 '[control_618]' is not marked as EOG\n",
            "load: control token:    621 '[control_619]' is not marked as EOG\n",
            "load: control token:    622 '[control_620]' is not marked as EOG\n",
            "load: control token:    770 '[control_768]' is not marked as EOG\n",
            "load: control token:    623 '[control_621]' is not marked as EOG\n",
            "load: control token:    625 '[control_623]' is not marked as EOG\n",
            "load: control token:    626 '[control_624]' is not marked as EOG\n",
            "load: control token:    627 '[control_625]' is not marked as EOG\n",
            "load: control token:    628 '[control_626]' is not marked as EOG\n",
            "load: control token:    629 '[control_627]' is not marked as EOG\n",
            "load: control token:    763 '[control_761]' is not marked as EOG\n",
            "load: control token:    630 '[control_628]' is not marked as EOG\n",
            "load: control token:    762 '[control_760]' is not marked as EOG\n",
            "load: control token:    631 '[control_629]' is not marked as EOG\n",
            "load: control token:    632 '[control_630]' is not marked as EOG\n",
            "load: control token:    633 '[control_631]' is not marked as EOG\n",
            "load: control token:    634 '[control_632]' is not marked as EOG\n",
            "load: control token:    635 '[control_633]' is not marked as EOG\n",
            "load: control token:    636 '[control_634]' is not marked as EOG\n",
            "load: control token:    637 '[control_635]' is not marked as EOG\n",
            "load: control token:    638 '[control_636]' is not marked as EOG\n",
            "load: control token:    721 '[control_719]' is not marked as EOG\n",
            "load: control token:    639 '[control_637]' is not marked as EOG\n",
            "load: control token:    720 '[control_718]' is not marked as EOG\n",
            "load: control token:    719 '[control_717]' is not marked as EOG\n",
            "load: control token:    640 '[control_638]' is not marked as EOG\n",
            "load: control token:    718 '[control_716]' is not marked as EOG\n",
            "load: control token:    641 '[control_639]' is not marked as EOG\n",
            "load: control token:    642 '[control_640]' is not marked as EOG\n",
            "load: control token:    643 '[control_641]' is not marked as EOG\n",
            "load: control token:    644 '[control_642]' is not marked as EOG\n",
            "load: control token:    645 '[control_643]' is not marked as EOG\n",
            "load: control token:    646 '[control_644]' is not marked as EOG\n",
            "load: control token:    647 '[control_645]' is not marked as EOG\n",
            "load: control token:    648 '[control_646]' is not marked as EOG\n",
            "load: control token:    649 '[control_647]' is not marked as EOG\n",
            "load: control token:    650 '[control_648]' is not marked as EOG\n",
            "load: control token:    651 '[control_649]' is not marked as EOG\n",
            "load: control token:    652 '[control_650]' is not marked as EOG\n",
            "load: control token:    653 '[control_651]' is not marked as EOG\n",
            "load: control token:    654 '[control_652]' is not marked as EOG\n",
            "load: control token:    658 '[control_656]' is not marked as EOG\n",
            "load: control token:    659 '[control_657]' is not marked as EOG\n",
            "load: control token:    670 '[control_668]' is not marked as EOG\n",
            "load: control token:    671 '[control_669]' is not marked as EOG\n",
            "load: control token:    672 '[control_670]' is not marked as EOG\n",
            "load: control token:    673 '[control_671]' is not marked as EOG\n",
            "load: control token:    674 '[control_672]' is not marked as EOG\n",
            "load: control token:    675 '[control_673]' is not marked as EOG\n",
            "load: control token:    676 '[control_674]' is not marked as EOG\n",
            "load: control token:    677 '[control_675]' is not marked as EOG\n",
            "load: control token:    678 '[control_676]' is not marked as EOG\n",
            "load: control token:    679 '[control_677]' is not marked as EOG\n",
            "load: control token:    680 '[control_678]' is not marked as EOG\n",
            "load: control token:    681 '[control_679]' is not marked as EOG\n",
            "load: control token:    682 '[control_680]' is not marked as EOG\n",
            "load: control token:    683 '[control_681]' is not marked as EOG\n",
            "load: control token:    684 '[control_682]' is not marked as EOG\n",
            "load: control token:    685 '[control_683]' is not marked as EOG\n",
            "load: control token:    688 '[control_686]' is not marked as EOG\n",
            "load: control token:    689 '[control_687]' is not marked as EOG\n",
            "load: control token:    700 '[control_698]' is not marked as EOG\n",
            "load: control token:    701 '[control_699]' is not marked as EOG\n",
            "load: control token:    704 '[control_702]' is not marked as EOG\n",
            "load: control token:    705 '[control_703]' is not marked as EOG\n",
            "load: control token:    706 '[control_704]' is not marked as EOG\n",
            "load: control token:    707 '[control_705]' is not marked as EOG\n",
            "load: control token:    708 '[control_706]' is not marked as EOG\n",
            "load: control token:    709 '[control_707]' is not marked as EOG\n",
            "load: control token:    712 '[control_710]' is not marked as EOG\n",
            "load: control token:    713 '[control_711]' is not marked as EOG\n",
            "load: control token:    714 '[control_712]' is not marked as EOG\n",
            "load: control token:    715 '[control_713]' is not marked as EOG\n",
            "load: control token:    716 '[control_714]' is not marked as EOG\n",
            "load: control token:    717 '[control_715]' is not marked as EOG\n",
            "load: control token:    722 '[control_720]' is not marked as EOG\n",
            "load: control token:    723 '[control_721]' is not marked as EOG\n",
            "load: control token:    726 '[control_724]' is not marked as EOG\n",
            "load: control token:    727 '[control_725]' is not marked as EOG\n",
            "load: control token:    728 '[control_726]' is not marked as EOG\n",
            "load: control token:    729 '[control_727]' is not marked as EOG\n",
            "load: control token:    734 '[control_732]' is not marked as EOG\n",
            "load: control token:    735 '[control_733]' is not marked as EOG\n",
            "load: control token:    736 '[control_734]' is not marked as EOG\n",
            "load: control token:    737 '[control_735]' is not marked as EOG\n",
            "load: control token:    738 '[control_736]' is not marked as EOG\n",
            "load: control token:    739 '[control_737]' is not marked as EOG\n",
            "load: control token:    742 '[control_740]' is not marked as EOG\n",
            "load: control token:    743 '[control_741]' is not marked as EOG\n",
            "load: control token:    744 '[control_742]' is not marked as EOG\n",
            "load: control token:    745 '[control_743]' is not marked as EOG\n",
            "load: control token:    746 '[control_744]' is not marked as EOG\n",
            "load: control token:    747 '[control_745]' is not marked as EOG\n",
            "load: control token:    748 '[control_746]' is not marked as EOG\n",
            "load: control token:    749 '[control_747]' is not marked as EOG\n",
            "load: control token:    750 '[control_748]' is not marked as EOG\n",
            "load: control token:    751 '[control_749]' is not marked as EOG\n",
            "load: control token:    752 '[control_750]' is not marked as EOG\n",
            "load: control token:    753 '[control_751]' is not marked as EOG\n",
            "load: control token:    754 '[control_752]' is not marked as EOG\n",
            "load: control token:    755 '[control_753]' is not marked as EOG\n",
            "load: control token:    756 '[control_754]' is not marked as EOG\n",
            "load: control token:    757 '[control_755]' is not marked as EOG\n",
            "load: control token:    764 '[control_762]' is not marked as EOG\n",
            "load: control token:    765 '[control_763]' is not marked as EOG\n",
            "load: control token:    766 '[control_764]' is not marked as EOG\n",
            "load: control token:    767 '[control_765]' is not marked as EOG\n",
            "load: control token:    768 '[control_766]' is not marked as EOG\n",
            "load: control token:    769 '[control_767]' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 771\n",
            "load: token to piece cache size = 0.1731 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 14336\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 7.25 B\n",
            "print_info: general.name     = Mistral7B Finetuned\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32768\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: PAD token        = 770 '[control_768]'\n",
            "print_info: LF token         = 781 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q4_K) (and 98 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors:   CPU_REPACK model buffer size =  3204.00 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  4138.02 MiB\n",
            "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.10.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.11.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.27.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.28.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.28.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.28.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.29.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.29.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.29.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.30.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.30.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.30.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.31.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.31.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.31.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
            "...................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 1024\n",
            "llama_context: n_ctx_per_seq = 1024\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.12 MiB\n",
            "create_memory: n_ctx = 1024 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified: layer  28: dev = CPU\n",
            "llama_kv_cache_unified: layer  29: dev = CPU\n",
            "llama_kv_cache_unified: layer  30: dev = CPU\n",
            "llama_kv_cache_unified: layer  31: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   128.00 MiB\n",
            "llama_kv_cache_unified: size =  128.00 MiB (  1024 cells,  32 layers,  1 seqs), K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
            "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 65536\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =   110.01 MiB\n",
            "llama_context: graph nodes  = 1158\n",
            "llama_context: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'general.name': 'Mistral7B Finetuned', 'general.architecture': 'llama', 'general.type': 'model', 'llama.feed_forward_length': '14336', 'general.size_label': '7.2B', 'llama.block_count': '32', 'llama.context_length': '32768', 'llama.embedding_length': '4096', 'llama.attention.head_count': '32', 'general.file_type': '15', 'tokenizer.ggml.eos_token_id': '2', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '1000000.000000', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.attention.key_length': '128', 'llama.attention.value_length': '128', 'llama.vocab_size': '32768', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'llama', 'tokenizer.ggml.add_space_prefix': 'true', 'tokenizer.ggml.pre': 'default', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '770', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_sep_token': 'false', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': '{%- if messages[0][\"role\"] == \"system\" %}\\n    {%- set system_message = messages[0][\"content\"] %}\\n    {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\\n\\n{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\\n{%- set ns = namespace() %}\\n{%- set ns.index = 0 %}\\n{%- for message in loop_messages %}\\n    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\\n        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\\n            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\\n        {%- endif %}\\n        {%- set ns.index = ns.index + 1 %}\\n    {%- endif %}\\n{%- endfor %}\\n\\n{{- bos_token }}\\n{%- for message in loop_messages %}\\n    {%- if message[\"role\"] == \"user\" %}\\n        {%- if tools is not none and (message == user_messages[-1]) %}\\n            {{- \"[AVAILABLE_TOOLS] [\" }}\\n            {%- for tool in tools %}\\n                {%- set tool = tool.function %}\\n                {{- \\'{\"type\": \"function\", \"function\": {\\' }}\\n                {%- for key, val in tool.items() if key != \"return\" %}\\n                    {%- if val is string %}\\n                        {{- \\'\"\\' + key + \\'\": \"\\' + val + \\'\"\\' }}\\n                    {%- else %}\\n                        {{- \\'\"\\' + key + \\'\": \\' + val|tojson }}\\n                    {%- endif %}\\n                    {%- if not loop.last %}\\n                        {{- \", \" }}\\n                    {%- endif %}\\n                {%- endfor %}\\n                {{- \"}}\" }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- else %}\\n                    {{- \"]\" }}\\n                {%- endif %}\\n            {%- endfor %}\\n            {{- \"[/AVAILABLE_TOOLS]\" }}\\n            {%- endif %}\\n        {%- if loop.last and system_message is defined %}\\n            {{- \"[INST] \" + system_message + \"\\\\n\\\\n\" + message[\"content\"] + \"[/INST]\" }}\\n        {%- else %}\\n            {{- \"[INST] \" + message[\"content\"] + \"[/INST]\" }}\\n        {%- endif %}\\n    {%- elif message.tool_calls is defined and message.tool_calls is not none %}\\n        {{- \"[TOOL_CALLS] [\" }}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- set out = tool_call.function|tojson %}\\n            {{- out[:-1] }}\\n            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\\n                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\\n            {%- endif %}\\n            {{- \\', \"id\": \"\\' + tool_call.id + \\'\"}\\' }}\\n            {%- if not loop.last %}\\n                {{- \", \" }}\\n            {%- else %}\\n                {{- \"]\" + eos_token }}\\n            {%- endif %}\\n        {%- endfor %}\\n    {%- elif message[\"role\"] == \"assistant\" %}\\n        {{- \" \" + message[\"content\"]|trim + eos_token}}\\n    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\\n        {%- if message.content is defined and message.content.content is defined %}\\n            {%- set content = message.content.content %}\\n        {%- else %}\\n            {%- set content = message.content %}\\n        {%- endif %}\\n        {{- \\'[TOOL_RESULTS] {\"content\": \\' + content|string + \", \" }}\\n        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\\n            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\\n        {%- endif %}\\n        {{- \\'\"call_id\": \"\\' + message.tool_call_id + \\'\"}[/TOOL_RESULTS]\\' }}\\n    {%- else %}\\n        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\\n    {%- endif %}\\n{%- endfor %}\\n'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {%- if messages[0][\"role\"] == \"system\" %}\n",
            "    {%- set system_message = messages[0][\"content\"] %}\n",
            "    {%- set loop_messages = messages[1:] %}\n",
            "{%- else %}\n",
            "    {%- set loop_messages = messages %}\n",
            "{%- endif %}\n",
            "{%- if not tools is defined %}\n",
            "    {%- set tools = none %}\n",
            "{%- endif %}\n",
            "{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\n",
            "\n",
            "{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n",
            "{%- set ns = namespace() %}\n",
            "{%- set ns.index = 0 %}\n",
            "{%- for message in loop_messages %}\n",
            "    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n",
            "        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\n",
            "            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
            "        {%- endif %}\n",
            "        {%- set ns.index = ns.index + 1 %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "\n",
            "{{- bos_token }}\n",
            "{%- for message in loop_messages %}\n",
            "    {%- if message[\"role\"] == \"user\" %}\n",
            "        {%- if tools is not none and (message == user_messages[-1]) %}\n",
            "            {{- \"[AVAILABLE_TOOLS] [\" }}\n",
            "            {%- for tool in tools %}\n",
            "                {%- set tool = tool.function %}\n",
            "                {{- '{\"type\": \"function\", \"function\": {' }}\n",
            "                {%- for key, val in tool.items() if key != \"return\" %}\n",
            "                    {%- if val is string %}\n",
            "                        {{- '\"' + key + '\": \"' + val + '\"' }}\n",
            "                    {%- else %}\n",
            "                        {{- '\"' + key + '\": ' + val|tojson }}\n",
            "                    {%- endif %}\n",
            "                    {%- if not loop.last %}\n",
            "                        {{- \", \" }}\n",
            "                    {%- endif %}\n",
            "                {%- endfor %}\n",
            "                {{- \"}}\" }}\n",
            "                {%- if not loop.last %}\n",
            "                    {{- \", \" }}\n",
            "                {%- else %}\n",
            "                    {{- \"]\" }}\n",
            "                {%- endif %}\n",
            "            {%- endfor %}\n",
            "            {{- \"[/AVAILABLE_TOOLS]\" }}\n",
            "            {%- endif %}\n",
            "        {%- if loop.last and system_message is defined %}\n",
            "            {{- \"[INST] \" + system_message + \"\\n\\n\" + message[\"content\"] + \"[/INST]\" }}\n",
            "        {%- else %}\n",
            "            {{- \"[INST] \" + message[\"content\"] + \"[/INST]\" }}\n",
            "        {%- endif %}\n",
            "    {%- elif message.tool_calls is defined and message.tool_calls is not none %}\n",
            "        {{- \"[TOOL_CALLS] [\" }}\n",
            "        {%- for tool_call in message.tool_calls %}\n",
            "            {%- set out = tool_call.function|tojson %}\n",
            "            {{- out[:-1] }}\n",
            "            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n",
            "                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n",
            "            {%- endif %}\n",
            "            {{- ', \"id\": \"' + tool_call.id + '\"}' }}\n",
            "            {%- if not loop.last %}\n",
            "                {{- \", \" }}\n",
            "            {%- else %}\n",
            "                {{- \"]\" + eos_token }}\n",
            "            {%- endif %}\n",
            "        {%- endfor %}\n",
            "    {%- elif message[\"role\"] == \"assistant\" %}\n",
            "        {{- \" \" + message[\"content\"]|trim + eos_token}}\n",
            "    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\n",
            "        {%- if message.content is defined and message.content.content is defined %}\n",
            "            {%- set content = message.content.content %}\n",
            "        {%- else %}\n",
            "            {%- set content = message.content %}\n",
            "        {%- endif %}\n",
            "        {{- '[TOOL_RESULTS] {\"content\": ' + content|string + \", \" }}\n",
            "        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n",
            "            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n",
            "        {%- endif %}\n",
            "        {{- '\"call_id\": \"' + message.tool_call_id + '\"}[/TOOL_RESULTS]' }}\n",
            "    {%- else %}\n",
            "        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "\n",
            "Using chat eos_token: </s>\n",
            "Using chat bos_token: <s>\n",
            "llama_perf_context_print:        load time =   17955.14 ms\n",
            "llama_perf_context_print: prompt eval time =   17938.17 ms /   191 tokens (   93.92 ms per token,    10.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =  395323.52 ms /    90 runs   ( 4392.48 ms per token,     0.23 tokens per second)\n",
            "llama_perf_context_print:       total time =  415527.92 ms /   281 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SQLSELECT SUM(spending) FROM defense.eu_humanitarian_assistance WHERE year >= 2019 AND year <= 2021;\n",
            "\n",
            "Explanation:This query calculates the total spending on humanitarian assistance by the European Union in the last 3 years by summing the spending column for rows where the year is within the range 2019 to 2021.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "llm = Llama(model_path=\"model_gguf/unsloth.Q4_K_M.gguf\",\n",
        "            n_ctx=1024,\n",
        "            n_gpu_layers=25,\n",
        "            use_gpu=True)\n",
        "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Company database: CREATE SCHEMA if not exists defense; CREATE TABLE if not exists eu_humanitarian_assistance (id INT PRIMARY KEY, year INT, spending INT); INSERT INTO defense.eu_humanitarian_assistance (id, year, spending) VALUES (1, 2019, 1500), (2, 2020, 1800), (3, 2021, 2100);\n",
        "\n",
        "### Input:\n",
        "SQL Prompt: What is the total spending on humanitarian assistance by the European Union in the last 3 years?\n",
        "\n",
        "### Response:\n",
        "\n",
        "### Explanation:\n",
        "\"\"\"\n",
        "\n",
        "output = llm(prompt, max_tokens=200, temperature=0.7)\n",
        "print(output[\"choices\"][0][\"text\"])\n",
        "\n",
        "#doesnt use gpuuuu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348,
          "referenced_widgets": [
            "3672c5728e8943d99d631c2adbe3b84b",
            "96f5753c2ef04d82a942545c8c4c5ac1",
            "29b503439a40432584c418589691ec60",
            "38874065010b446586694019fcc8da52",
            "373dcc8e11e5460e952b9d0a1a3054ae",
            "7f35fe372fbd49779e431912ee839c3e",
            "8f33ed43d5964b67a9fb6c0079d97296",
            "cf824c2986914f2f9965c5c32002fda3",
            "45f94583b21d4ebe97b282bf2f99a986",
            "7e9da87a4bf14b5a975b1f18b081cdc9",
            "c2633fac3c61419ca009e895562ea158",
            "1cf0c38b47aa4e72a88d224174306333",
            "74e84be9f63c4386883b63379bd62ae8",
            "dbab1dcadc614da587fe0424bb30732c",
            "1c009397e3164b6ebb477dd60139cf47",
            "059f7695090f46f3adbe8e66b9de0669",
            "bb3e894430c94544ac0ad8c5668cfeb6",
            "65051dfa44cc4adbb1ddf92cc7cb7feb",
            "8b94c254b4ed4284bcd49097894474f9",
            "34aecf27c73b48c2854350c7c5d6f76c",
            "febee3c9dd344a04b0c7d103d9687674",
            "05d6a34b73b94335a638b2bc594c7f0a",
            "47382d7e2f934c79885bdcf220a224e5",
            "a1144a5e65b044739ba124fc65f3e7d6",
            "2d7f514dcbfb41a8b42842650c608d08",
            "053b717e1c9f4f1bb76259beb198be64",
            "2669aa970667453ea318b98f4454b102",
            "5fe4508d94e0469cbb55469d07f89a4a",
            "04a17bebdf6441449b489ed0f5a96faa",
            "c642833c616e478daa6be50d8203f175",
            "20547d61e14645f4b371e9c8a88eec06",
            "12e352100f9c4fc184ae1e81310f3630",
            "172945ff255f48c39a08df117c6d84c4",
            "7b3121f9b18f4da38c9e618500681d5c",
            "6756d6d19a0b4db6a20645678a3ff6ed",
            "7efbc12e62bb4abe805a46ea8d1a43fd",
            "5142313083b048008685f2369a2cbd16",
            "72e268ee2f5f44c381a0eed32277481d",
            "1a3de63358764c5d8f6546e938c3f838",
            "37575c6de34e4ec5ac1a70f6caf46f56",
            "d260185d09444f4fb04d12661454d0a3",
            "d14074b8173644b0a2a5f8f062b4121e",
            "418e1d454b3f46f996fbace3e58515c8",
            "378b726d41eb43718fda496edc5ebb83",
            "728a58c1b0554d659d06b98af57c6015",
            "dbe6d9d665d34412b8c44af44b141d5a",
            "75148315e95b4837a103902ff0aa92e7",
            "f66def79bf814b66bef5ab229f0e3fb7",
            "ffe53547921a409796f436c30c7ea1db",
            "e72af7ffc71b41e58b7a4666491fac2d",
            "ef085c1d949745b6a8c2163d9de6fa65",
            "5d816152ca6e4ed68b3a8cf8e096f175",
            "d26098cccaad400886025ba4d5e1c7a4",
            "12ea65dc39b74639b3722b98992d4f2b",
            "333c67fc507140bab24a381f83692753",
            "c2fcce6b3d0a441db1eb5e4979ac82fa",
            "db7e779cd5654a3bba68b9ee85b77275",
            "9f30118cf54848b9bb571b7bcb21fc34",
            "fc38b8e4bafc4669b64aaa10cc4efaff",
            "ec475858b32642828d622dd3e644fbc9",
            "73ac974d40c04bceae57af2b12ac2c6f",
            "2138a198805b4810a02fee1b8cd2b8e0",
            "92578e142b604e23aa080dbd403804a3",
            "324f2ead30fd481184225594392ad6ae",
            "fee6b86a727346849b50ef6f845da597",
            "971c711970e34aaeb5ed44bfb3eb0925"
          ]
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "31c55902-267c-4b0f-99a0-2246b2319d39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.8.1: Fast Mistral patching. Transformers: 4.54.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3672c5728e8943d99d631c2adbe3b84b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/4.14G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1cf0c38b47aa4e72a88d224174306333",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/157 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47382d7e2f934c79885bdcf220a224e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b3121f9b18f4da38c9e618500681d5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "728a58c1b0554d659d06b98af57c6015",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/446 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2fcce6b3d0a441db1eb5e4979ac82fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "d89eacfa-c03c-4494-b8c8-19820eb95662"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.8.1 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3LEIHb63oegK"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt=\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Company database: {}\n",
        "\n",
        "### Input:\n",
        "SQL Prompt: {}\n",
        "\n",
        "###Response:\n",
        "SQL{}\n",
        "\n",
        "Explanation:{}\n",
        "\"\"\"\n",
        "\n",
        "EOS_TOKEN=tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "  company_databases=examples[\"sql_context\"]\n",
        "  prompts=examples[\"sql_prompt\"]\n",
        "  sqls=examples[\"sql\"]\n",
        "  explanations=examples[\"sql_explanation\"]\n",
        "  texts=[]\n",
        "  for company_database,prompt,sql,explanation in zip(company_databases,prompts,sqls,explanations):\n",
        "    #Must add eos_token, otherwise geenration will go on forever\n",
        "    text=alpaca_prompt.format(company_database,prompt,sql,explanation) + EOS_TOKEN\n",
        "    texts.append(text)\n",
        "  return {\"text\":texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset=load_dataset(\"gretelai/synthetic_text_to_sql\",split=\"train\")\n",
        "dataset=dataset.map(formatting_prompts_func,batched=True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "_KI0M54iq3BW",
        "outputId": "7b28e794-85db-461b-a328-f3bf4bbebe0e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCompany database: CREATE SCHEMA if not exists defense; CREATE TABLE if not exists eu_humanitarian_assistance (id INT PRIMARY KEY, year INT, spending INT); INSERT INTO defense.eu_humanitarian_assistance (id, year, spending) VALUES (1, 2019, 1500), (2, 2020, 1800), (3, 2021, 2100);\\n\\n### Input:\\nSQL Prompt: What is the total spending on humanitarian assistance by the European Union in the last 3 years?\\n\\n###Response:\\nSQLSELECT SUM(spending) FROM defense.eu_humanitarian_assistance WHERE year BETWEEN 2019 AND 2021;\\n\\nExplanation:This SQL query calculates the total spending on humanitarian assistance by the European Union in the last 3 years. It uses the SUM() aggregation function to find the total spending and WHERE clause to filter the result by year.\\n</s>'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['text'][5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95_Nn-89DhsL",
        "outputId": "c532f6a9-46b6-4d38-844b-cdca0470f440"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field = \"text\", #hena ba bey create el input ids w el labels w kda gowah malnash da3wa (use this better always)\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1,  # Set this for 1 full training run.\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "yqxqAZ7KJ4oL",
        "outputId": "77d4f0cc-de7f-4186-b333-c7827d7f16d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 100,000 | Num Epochs = 1 | Total steps = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 41,943,040 of 7,289,966,592 (0.58% trained)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 08:51, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.370500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.381800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.368100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.339500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.301000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.318300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.322900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.332300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.334900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.358500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.276100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.329100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.306600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.290800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.296300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.369100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.292100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.318700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.356300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.340400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.337200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.305300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.318900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.327300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.290900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.326100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.331700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.331200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.327800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.321100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.372700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.344500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.328000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.405700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.356300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.339200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.337500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.323600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.313500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.329600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.367200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.360300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.347100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.309100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.366000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.347600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.329500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.333400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.334300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.379200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.326700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.348500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.376600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.453000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.460400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.380000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.383800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.316600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.360900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.378800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "3a267303-f04a-439f-f975-7d1dafc486a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SQLSELECT SUM(spending) FROM defense.eu_humanitarian_assistance WHERE year >= (SELECT YEAR(CURRENT_DATE()) - 3);\n",
            "\n",
            "Explanation:This query calculates the total spending on humanitarian assistance by the European Union in the last 3 years by summing the spending column in the eu_humanitarian_assistance table where the year is greater than or equal to 3 years before the current date.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Company database: CREATE SCHEMA if not exists defense; CREATE TABLE if not exists eu_humanitarian_assistance (id INT PRIMARY KEY, year INT, spending INT); INSERT INTO defense.eu_humanitarian_assistance (id, year, spending) VALUES (1, 2019, 1500), (2, 2020, 1800), (3, 2021, 2100);\n",
        "\n",
        "### Input:\n",
        "SQL Prompt: What is the total spending on humanitarian assistance by the European Union in the last 3 years?\n",
        "\n",
        "### Response:\n",
        "\n",
        "### Explanation:\n",
        "\"\"\"\n",
        "FastLanguageModel.for_inference(model) #with sloth You don’t need to manually load adapters or merge anything after training, modle is already ready.\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=False,\n",
        "    temperature=0.7\n",
        ")\n",
        "response=tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(response[len(prompt):])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "\n",
        "### Saving, loading finetuned models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upcOlWe7A1vc",
        "outputId": "ddd1fed6-b353-40fb-db58-c24c09174ade"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/chat_template.jinja',\n",
              " 'lora_model/tokenizer.model',\n",
              " 'lora_model/added_tokens.json',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"lora_model\")\n",
        "tokenizer.save_pretrained(\"lora_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fDokL7n-yJE"
      },
      "outputs": [],
      "source": [
        "# Merge to 4bit\n",
        "if True: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqfebeAdT073",
        "outputId": "887165fa-fa80-40fc-8a6a-665b3f30f0ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
            "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
            "To force `safe_serialization`, set it to `None` instead.\n",
            "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
            "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
            "Unsloth: Will remove a cached repo with size 4.1G\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 1.3 out of 12.67 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 59%|█████▉    | 19/32 [00:01<00:01, 12.87it/s]\n",
            "We will save to Disk and not RAM now.\n",
            "100%|██████████| 32/32 [07:50<00:00, 14.70s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving model_gguf/pytorch_model-00001-of-00003.bin...\n",
            "Unsloth: Saving model_gguf/pytorch_model-00002-of-00003.bin...\n",
            "Unsloth: Saving model_gguf/pytorch_model-00003-of-00003.bin...\n",
            "Done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Converting mistral model. Can use fast conversion = True.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: CMAKE detected. Finalizing some steps for installation.\n",
            "Unsloth: [1] Converting model at model_gguf into f16 GGUF format.\n",
            "The output location will be /content/model_gguf/unsloth.F16.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: model_gguf\n",
            "INFO:hf-to-gguf:Model architecture: MistralForCausalLM\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00003.bin'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {4096, 32768}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00003.bin'\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00003-of-00003.bin'\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:output.weight,               torch.float16 --> F16, shape = {4096, 32768}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 32768\n",
            "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 14336\n",
            "INFO:hf-to-gguf:gguf: head count = 32\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
            "INFO:hf-to-gguf:gguf: rope theta = 1000000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "WARNING:gguf.vocab:Unknown separator token '<s>' in TemplateProcessing<pair>\n",
            "INFO:gguf.vocab:Setting special token type bos to 1\n",
            "INFO:gguf.vocab:Setting special token type eos to 2\n",
            "INFO:gguf.vocab:Setting special token type unk to 0\n",
            "INFO:gguf.vocab:Setting special token type pad to 770\n",
            "INFO:gguf.vocab:Setting add_bos_token to True\n",
            "INFO:gguf.vocab:Setting add_sep_token to False\n",
            "INFO:gguf.vocab:Setting add_eos_token to False\n",
            "INFO:gguf.vocab:Setting chat_template to {%- if messages[0][\"role\"] == \"system\" %}\n",
            "    {%- set system_message = messages[0][\"content\"] %}\n",
            "    {%- set loop_messages = messages[1:] %}\n",
            "{%- else %}\n",
            "    {%- set loop_messages = messages %}\n",
            "{%- endif %}\n",
            "{%- if not tools is defined %}\n",
            "    {%- set tools = none %}\n",
            "{%- endif %}\n",
            "{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\n",
            "\n",
            "{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n",
            "{%- set ns = namespace() %}\n",
            "{%- set ns.index = 0 %}\n",
            "{%- for message in loop_messages %}\n",
            "    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n",
            "        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\n",
            "            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
            "        {%- endif %}\n",
            "        {%- set ns.index = ns.index + 1 %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "\n",
            "{{- bos_token }}\n",
            "{%- for message in loop_messages %}\n",
            "    {%- if message[\"role\"] == \"user\" %}\n",
            "        {%- if tools is not none and (message == user_messages[-1]) %}\n",
            "            {{- \"[AVAILABLE_TOOLS] [\" }}\n",
            "            {%- for tool in tools %}\n",
            "                {%- set tool = tool.function %}\n",
            "                {{- '{\"type\": \"function\", \"function\": {' }}\n",
            "                {%- for key, val in tool.items() if key != \"return\" %}\n",
            "                    {%- if val is string %}\n",
            "                        {{- '\"' + key + '\": \"' + val + '\"' }}\n",
            "                    {%- else %}\n",
            "                        {{- '\"' + key + '\": ' + val|tojson }}\n",
            "                    {%- endif %}\n",
            "                    {%- if not loop.last %}\n",
            "                        {{- \", \" }}\n",
            "                    {%- endif %}\n",
            "                {%- endfor %}\n",
            "                {{- \"}}\" }}\n",
            "                {%- if not loop.last %}\n",
            "                    {{- \", \" }}\n",
            "                {%- else %}\n",
            "                    {{- \"]\" }}\n",
            "                {%- endif %}\n",
            "            {%- endfor %}\n",
            "            {{- \"[/AVAILABLE_TOOLS]\" }}\n",
            "            {%- endif %}\n",
            "        {%- if loop.last and system_message is defined %}\n",
            "            {{- \"[INST] \" + system_message + \"\\n\\n\" + message[\"content\"] + \"[/INST]\" }}\n",
            "        {%- else %}\n",
            "            {{- \"[INST] \" + message[\"content\"] + \"[/INST]\" }}\n",
            "        {%- endif %}\n",
            "    {%- elif message.tool_calls is defined and message.tool_calls is not none %}\n",
            "        {{- \"[TOOL_CALLS] [\" }}\n",
            "        {%- for tool_call in message.tool_calls %}\n",
            "            {%- set out = tool_call.function|tojson %}\n",
            "            {{- out[:-1] }}\n",
            "            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n",
            "                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n",
            "            {%- endif %}\n",
            "            {{- ', \"id\": \"' + tool_call.id + '\"}' }}\n",
            "            {%- if not loop.last %}\n",
            "                {{- \", \" }}\n",
            "            {%- else %}\n",
            "                {{- \"]\" + eos_token }}\n",
            "            {%- endif %}\n",
            "        {%- endfor %}\n",
            "    {%- elif message[\"role\"] == \"assistant\" %}\n",
            "        {{- \" \" + message[\"content\"]|trim + eos_token}}\n",
            "    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\n",
            "        {%- if message.content is defined and message.content.content is defined %}\n",
            "            {%- set content = message.content.content %}\n",
            "        {%- else %}\n",
            "            {%- set content = message.content %}\n",
            "        {%- endif %}\n",
            "        {{- '[TOOL_RESULTS] {\"content\": ' + content|string + \", \" }}\n",
            "        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n",
            "            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n",
            "        {%- endif %}\n",
            "        {{- '\"call_id\": \"' + message.tool_call_id + '\"}[/TOOL_RESULTS]' }}\n",
            "    {%- else %}\n",
            "        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/model_gguf/unsloth.F16.gguf: n_tensors = 291, total_size = 14.5G\n",
            "Writing: 100%|██████████| 14.5G/14.5G [22:20<00:00, 10.8Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/model_gguf/unsloth.F16.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/model_gguf/unsloth.F16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
            "main: build = 6096 (fd1234cb)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/model_gguf/unsloth.F16.gguf' to '/content/model_gguf/unsloth.Q4_K_M.gguf' as Q4_K_M using 4 threads\n",
            "llama_model_loader: loaded meta data with 32 key-value pairs and 291 tensors from /content/model_gguf/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Model_Gguf\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 7.2B\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  14:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 32768\n",
            "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\n",
            "llama_model_loader: - kv  21:                      tokenizer.ggml.scores arr[f32,32768]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,32768]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  25:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  26:            tokenizer.ggml.padding_token_id u32              = 770\n",
            "llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  28:               tokenizer.ggml.add_sep_token bool             = false\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {%- if messages[0][\"role\"] == \"system...\n",
            "llama_model_loader: - kv  31:            tokenizer.ggml.add_space_prefix bool             = true\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:  226 tensors\n",
            "[   1/ 291]                        output.weight - [ 4096, 32768,     1,     1], type =    f16, converting to q6_K .. size =   256.00 MiB ->   105.00 MiB\n",
            "[   2/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   3/ 291]                    token_embd.weight - [ 4096, 32768,     1,     1], type =    f16, converting to q4_K .. size =   256.00 MiB ->    72.00 MiB\n",
            "[   4/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[   5/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   6/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   7/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   8/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[   9/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  10/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  11/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  12/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  13/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  14/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  15/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  16/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  17/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  18/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  19/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  20/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  21/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  22/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  23/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  24/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  25/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  26/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  27/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  28/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  29/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  30/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  31/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  32/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  33/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  34/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  35/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  36/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  37/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  38/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  39/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  40/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  41/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  42/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  43/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  44/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  45/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  46/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  47/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  48/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  49/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  50/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  51/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  52/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  53/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  54/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  55/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  56/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  57/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  58/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  59/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  60/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  61/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  62/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  63/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  64/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  65/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  66/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  67/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  68/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  69/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  70/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  71/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  72/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  73/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  74/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  75/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  76/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  77/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  78/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  79/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  80/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  81/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  82/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  83/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  84/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  85/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  86/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  87/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  88/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  89/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  90/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  91/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  92/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  93/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  94/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  95/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  96/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  97/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  98/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  99/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 100/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 101/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 102/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 103/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 104/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 105/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 106/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 107/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 108/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 109/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 110/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 111/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 112/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 113/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 114/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 115/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 116/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 117/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 118/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 119/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 120/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 121/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 122/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 123/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 124/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 125/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 126/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 127/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 128/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 129/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 130/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 131/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 132/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 133/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 134/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 135/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 136/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 137/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 138/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 139/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 140/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 141/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 142/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 143/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 144/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 145/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 146/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 147/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 148/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 149/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 150/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 151/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 152/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 153/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 154/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 155/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 156/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 157/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 158/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 159/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 160/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 161/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 162/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 163/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 164/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 165/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 166/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 167/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 168/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 169/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 170/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 171/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 172/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 173/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 174/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 175/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 176/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 177/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 178/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 179/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 180/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 181/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 182/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 183/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 184/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 185/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 186/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 187/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 188/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 189/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 190/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 191/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 192/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 193/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 194/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 195/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 196/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 197/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 198/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 199/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 200/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 201/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 202/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 203/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 204/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 205/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 206/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 207/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 208/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 209/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 210/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 211/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 212/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 213/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 214/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 215/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 216/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 217/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 218/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 219/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 220/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 221/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 222/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 223/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 224/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 225/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 226/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 227/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 228/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 229/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 230/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 231/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 232/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 233/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 234/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 235/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 236/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 237/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 238/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 239/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 240/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 241/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 242/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 243/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 244/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 245/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 246/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 247/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 248/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 249/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 250/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 251/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 252/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 253/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 254/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 255/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 256/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 257/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 258/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 259/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 260/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 261/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 262/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 263/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 264/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 265/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 266/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 267/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 268/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 269/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 270/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 271/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 272/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 273/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 274/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 275/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 276/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 277/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 278/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 279/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 280/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 281/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 282/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 283/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 284/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 285/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 286/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 287/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 288/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 289/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 291/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "llama_model_quantize_impl: model size  = 13825.02 MB\n",
            "llama_model_quantize_impl: quant size  =  4169.52 MB\n",
            "\n",
            "main: quantize time = 1294752.01 ms\n",
            "main:    total time = 1294752.01 ms\n",
            "Unsloth: Conversion completed! Output location: /content/model_gguf/unsloth.Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "# Save to q4_k_m GGUF\n",
        "model.save_pretrained_gguf(\"model_gguf\", tokenizer, quantization_method = \"q4_k_m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.push_to_hub_gguf(\n",
        "    repo_id=\"Karimtawfik/mistral7b-finetuned\",\n",
        "    tokenizer=tokenizer,\n",
        "    quantization_method=\"q4_k_m\",\n",
        "    token=\"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\"      \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "c:\\Users\\karim\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\quantizers\\auto.py:222: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n"
          ]
        }
      ],
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    \"lora_model\",\n",
        "    load_in_4bit = True,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SQLSELECT SUM(spending) FROM defense.eu_humanitarian_assistance WHERE year >= (SELECT YEAR(CURRENT_DATE()) - 3);\n",
            "\n",
            "Explanation:This query calculates the total spending on humanitarian assistance by the European Union in the last 3 years by summing the spending column in the eu_humanitarian_assistance table where the year is greater than or equal to 3 years before the current date.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Company database: CREATE SCHEMA if not exists defense; CREATE TABLE if not exists eu_humanitarian_assistance (id INT PRIMARY KEY, year INT, spending INT); INSERT INTO defense.eu_humanitarian_assistance (id, year, spending) VALUES (1, 2019, 1500), (2, 2020, 1800), (3, 2021, 2100);\n",
        "\n",
        "### Input:\n",
        "SQL Prompt: What is the total spending on humanitarian assistance by the European Union in the last 3 years?\n",
        "\n",
        "### Response:\n",
        "\n",
        "### Explanation:\n",
        "\"\"\"\n",
        "# Tokenize\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "# Generate\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        temperature=0.7,\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(output_text[len(prompt):])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04a17bebdf6441449b489ed0f5a96faa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "053b717e1c9f4f1bb76259beb198be64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12e352100f9c4fc184ae1e81310f3630",
            "placeholder": "​",
            "style": "IPY_MODEL_172945ff255f48c39a08df117c6d84c4",
            "value": " 141k/? [00:00&lt;00:00, 5.65MB/s]"
          }
        },
        "059f7695090f46f3adbe8e66b9de0669": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05d6a34b73b94335a638b2bc594c7f0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12e352100f9c4fc184ae1e81310f3630": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12ea65dc39b74639b3722b98992d4f2b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "172945ff255f48c39a08df117c6d84c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a3de63358764c5d8f6546e938c3f838": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c009397e3164b6ebb477dd60139cf47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_febee3c9dd344a04b0c7d103d9687674",
            "placeholder": "​",
            "style": "IPY_MODEL_05d6a34b73b94335a638b2bc594c7f0a",
            "value": " 157/157 [00:00&lt;00:00, 7.78kB/s]"
          }
        },
        "1cf0c38b47aa4e72a88d224174306333": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74e84be9f63c4386883b63379bd62ae8",
              "IPY_MODEL_dbab1dcadc614da587fe0424bb30732c",
              "IPY_MODEL_1c009397e3164b6ebb477dd60139cf47"
            ],
            "layout": "IPY_MODEL_059f7695090f46f3adbe8e66b9de0669"
          }
        },
        "20547d61e14645f4b371e9c8a88eec06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2138a198805b4810a02fee1b8cd2b8e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2669aa970667453ea318b98f4454b102": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29b503439a40432584c418589691ec60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf824c2986914f2f9965c5c32002fda3",
            "max": 4138270819,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45f94583b21d4ebe97b282bf2f99a986",
            "value": 4138270819
          }
        },
        "2d7f514dcbfb41a8b42842650c608d08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c642833c616e478daa6be50d8203f175",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20547d61e14645f4b371e9c8a88eec06",
            "value": 1
          }
        },
        "324f2ead30fd481184225594392ad6ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "333c67fc507140bab24a381f83692753": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34aecf27c73b48c2854350c7c5d6f76c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3672c5728e8943d99d631c2adbe3b84b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_96f5753c2ef04d82a942545c8c4c5ac1",
              "IPY_MODEL_29b503439a40432584c418589691ec60",
              "IPY_MODEL_38874065010b446586694019fcc8da52"
            ],
            "layout": "IPY_MODEL_373dcc8e11e5460e952b9d0a1a3054ae"
          }
        },
        "373dcc8e11e5460e952b9d0a1a3054ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37575c6de34e4ec5ac1a70f6caf46f56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "378b726d41eb43718fda496edc5ebb83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38874065010b446586694019fcc8da52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e9da87a4bf14b5a975b1f18b081cdc9",
            "placeholder": "​",
            "style": "IPY_MODEL_c2633fac3c61419ca009e895562ea158",
            "value": " 4.14G/4.14G [01:05&lt;00:00, 54.7MB/s]"
          }
        },
        "418e1d454b3f46f996fbace3e58515c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45f94583b21d4ebe97b282bf2f99a986": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47382d7e2f934c79885bdcf220a224e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a1144a5e65b044739ba124fc65f3e7d6",
              "IPY_MODEL_2d7f514dcbfb41a8b42842650c608d08",
              "IPY_MODEL_053b717e1c9f4f1bb76259beb198be64"
            ],
            "layout": "IPY_MODEL_2669aa970667453ea318b98f4454b102"
          }
        },
        "5142313083b048008685f2369a2cbd16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_418e1d454b3f46f996fbace3e58515c8",
            "placeholder": "​",
            "style": "IPY_MODEL_378b726d41eb43718fda496edc5ebb83",
            "value": " 587k/587k [00:00&lt;00:00, 764kB/s]"
          }
        },
        "5d816152ca6e4ed68b3a8cf8e096f175": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fe4508d94e0469cbb55469d07f89a4a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65051dfa44cc4adbb1ddf92cc7cb7feb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6756d6d19a0b4db6a20645678a3ff6ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a3de63358764c5d8f6546e938c3f838",
            "placeholder": "​",
            "style": "IPY_MODEL_37575c6de34e4ec5ac1a70f6caf46f56",
            "value": "tokenizer.model: 100%"
          }
        },
        "728a58c1b0554d659d06b98af57c6015": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dbe6d9d665d34412b8c44af44b141d5a",
              "IPY_MODEL_75148315e95b4837a103902ff0aa92e7",
              "IPY_MODEL_f66def79bf814b66bef5ab229f0e3fb7"
            ],
            "layout": "IPY_MODEL_ffe53547921a409796f436c30c7ea1db"
          }
        },
        "72e268ee2f5f44c381a0eed32277481d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73ac974d40c04bceae57af2b12ac2c6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74e84be9f63c4386883b63379bd62ae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb3e894430c94544ac0ad8c5668cfeb6",
            "placeholder": "​",
            "style": "IPY_MODEL_65051dfa44cc4adbb1ddf92cc7cb7feb",
            "value": "generation_config.json: 100%"
          }
        },
        "75148315e95b4837a103902ff0aa92e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d816152ca6e4ed68b3a8cf8e096f175",
            "max": 446,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d26098cccaad400886025ba4d5e1c7a4",
            "value": 446
          }
        },
        "7b3121f9b18f4da38c9e618500681d5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6756d6d19a0b4db6a20645678a3ff6ed",
              "IPY_MODEL_7efbc12e62bb4abe805a46ea8d1a43fd",
              "IPY_MODEL_5142313083b048008685f2369a2cbd16"
            ],
            "layout": "IPY_MODEL_72e268ee2f5f44c381a0eed32277481d"
          }
        },
        "7e9da87a4bf14b5a975b1f18b081cdc9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7efbc12e62bb4abe805a46ea8d1a43fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d260185d09444f4fb04d12661454d0a3",
            "max": 587404,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d14074b8173644b0a2a5f8f062b4121e",
            "value": 587404
          }
        },
        "7f35fe372fbd49779e431912ee839c3e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b94c254b4ed4284bcd49097894474f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f33ed43d5964b67a9fb6c0079d97296": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92578e142b604e23aa080dbd403804a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "96f5753c2ef04d82a942545c8c4c5ac1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f35fe372fbd49779e431912ee839c3e",
            "placeholder": "​",
            "style": "IPY_MODEL_8f33ed43d5964b67a9fb6c0079d97296",
            "value": "model.safetensors: 100%"
          }
        },
        "971c711970e34aaeb5ed44bfb3eb0925": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f30118cf54848b9bb571b7bcb21fc34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92578e142b604e23aa080dbd403804a3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_324f2ead30fd481184225594392ad6ae",
            "value": 1
          }
        },
        "a1144a5e65b044739ba124fc65f3e7d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fe4508d94e0469cbb55469d07f89a4a",
            "placeholder": "​",
            "style": "IPY_MODEL_04a17bebdf6441449b489ed0f5a96faa",
            "value": "tokenizer_config.json: "
          }
        },
        "bb3e894430c94544ac0ad8c5668cfeb6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2633fac3c61419ca009e895562ea158": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2fcce6b3d0a441db1eb5e4979ac82fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db7e779cd5654a3bba68b9ee85b77275",
              "IPY_MODEL_9f30118cf54848b9bb571b7bcb21fc34",
              "IPY_MODEL_fc38b8e4bafc4669b64aaa10cc4efaff"
            ],
            "layout": "IPY_MODEL_ec475858b32642828d622dd3e644fbc9"
          }
        },
        "c642833c616e478daa6be50d8203f175": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "cf824c2986914f2f9965c5c32002fda3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d14074b8173644b0a2a5f8f062b4121e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d260185d09444f4fb04d12661454d0a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d26098cccaad400886025ba4d5e1c7a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db7e779cd5654a3bba68b9ee85b77275": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73ac974d40c04bceae57af2b12ac2c6f",
            "placeholder": "​",
            "style": "IPY_MODEL_2138a198805b4810a02fee1b8cd2b8e0",
            "value": "tokenizer.json: "
          }
        },
        "dbab1dcadc614da587fe0424bb30732c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b94c254b4ed4284bcd49097894474f9",
            "max": 157,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_34aecf27c73b48c2854350c7c5d6f76c",
            "value": 157
          }
        },
        "dbe6d9d665d34412b8c44af44b141d5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e72af7ffc71b41e58b7a4666491fac2d",
            "placeholder": "​",
            "style": "IPY_MODEL_ef085c1d949745b6a8c2163d9de6fa65",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "e72af7ffc71b41e58b7a4666491fac2d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec475858b32642828d622dd3e644fbc9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef085c1d949745b6a8c2163d9de6fa65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f66def79bf814b66bef5ab229f0e3fb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12ea65dc39b74639b3722b98992d4f2b",
            "placeholder": "​",
            "style": "IPY_MODEL_333c67fc507140bab24a381f83692753",
            "value": " 446/446 [00:00&lt;00:00, 14.9kB/s]"
          }
        },
        "fc38b8e4bafc4669b64aaa10cc4efaff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fee6b86a727346849b50ef6f845da597",
            "placeholder": "​",
            "style": "IPY_MODEL_971c711970e34aaeb5ed44bfb3eb0925",
            "value": " 1.96M/? [00:00&lt;00:00, 23.8MB/s]"
          }
        },
        "febee3c9dd344a04b0c7d103d9687674": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fee6b86a727346849b50ef6f845da597": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffe53547921a409796f436c30c7ea1db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
